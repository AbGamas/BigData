{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuración del entorno necesario para:\n",
    "* Cargar, explorar y procesar el dataset de calidad de vinos\n",
    "* Preparar características usando VectorAssembler y StandardScaler.\n",
    "* VectorAssembler sirve para combinar columnas de entrada en una sola columna de vectores.\n",
    "* StandardScaler asegura que la normalización tenga una varianza estándar de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis y Modelo de Regresión Logística para Predecir la Calidad del Vino\n",
    "# =========================================================================\n",
    "\n",
    "# ## Introducción\n",
    "# Este notebook está diseñado para cargar, explorar y preprocesar el dataset de calidad de vinos, y entrenar un modelo de regresión logística utilizando PySpark en un entorno de Databricks.\n",
    "\n",
    "# ## 1. Configuración de Databricks y Bibliotecas\n",
    "# Importamos las bibliotecas necesarias y configuramos la conexión con Databricks.\n",
    "\n",
    "import sys\n",
    "sys.executable\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear e iniciar una sesión en Spark. Es la forma para usar PySpark.\n",
    "Se inicia un entorno Spark local para procesar y analizar datos relacionados con la predicción de calidad de vinos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('WineQualityPrediction')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el csv.\n",
    "* inferSchema = True: Le dice a PySpark que deduzca automáticamente el tipo de datos de cada columna basado en el contenido del archivo.\n",
    "* sep=\";\": Especifica que el separador de columnas en el archivo es el punto y coma\n",
    "* header = True: Indica que la primera fila del archivo contiene los nombres de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. Carga de Datos\n",
    "# Cargamos el archivo CSV con los datos del vino y revisamos las primeras filas.\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "\n",
    "gt = spark.read.csv('data/winequality-red.csv', \n",
    "                       inferSchema = True,\n",
    "                       sep=\";\", \n",
    "                       header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se asigna el DataFrame gt a una nueva variable llamada df. Esto no crea una copia, sino que ambas variables (gt y df) apuntan al mismo DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La función withColumn se usa para crear o reemplazar una columna en el DataFrame. Aquí se crea una nueva columna llamada \"label\".\n",
    "* col(\"quality\") hace referencia a la columna \"quality\" en el DataFrame.\n",
    "* col(\"quality\") >= 6: Aplica una operación lógica: verifica si el valor de la columna \"quality\" es mayor o igual a 6.\n",
    "Esto genera una columna temporal con valores booleanos: True (si la condición se cumple) o False (si no se cumple).\n",
    "* .cast(\"int\"): Convierte los valores booleanos de la columna temporal en enteros: True se convierte en 1. False se convierte en 0.\n",
    "* df = ...:Reasigna el DataFrame df para incluir esta nueva columna \"label\".\n",
    "\n",
    "El propósito es transformar la columna \"quality\" en una nueva columna llamada \"label\", que representa una clasificación binaria:\n",
    "1 para vinos de calidad buena o excelente (calidad >= 6).\n",
    "0 para vinos de calidad regular o baja (calidad < 6).\n",
    "\n",
    "Esto prepara el dataset para ser usado en un modelo de clasificación, donde el objetivo será predecir la calidad del vino en dos clases (0 o 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna \"quality\" a una variable binaria en una columna llamada \"label\"\n",
    "\n",
    "df = df.withColumn(\"label\", (col(\"quality\") >= 6).cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* df.columns: Devuelve una lista con los nombres de todas las columnas del DataFrame df.\n",
    "* [c for c in df.columns]:Es una comprensión de lista que itera sobre cada columna c en df.columns.\n",
    "* if c not in [\"quality\", \"label\"]: Incluye solo aquellas columnas cuyo nombre no está en la lista [\"quality\", \"label\"].\n",
    "* Esto excluye específicamente:\n",
    "\"quality\": La columna original que contiene las calificaciones de calidad de los vinos.\n",
    "\"label\": La columna creada anteriormente para la clasificación binaria.\n",
    "* feature_columns = ...: Asigna la lista resultante a la variable feature_columns.\n",
    "\n",
    "El propósito es preparar los datos para el modelo. Se seleccionan las columnas relevantes como características, excluyendo:\n",
    "* \"quality\": Ya que fue utilizada para derivar la columna \"label\".\n",
    "* \"label\": Porque esta es la variable objetivo (target) del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de características\n",
    "\n",
    "feature_columns = [c for c in df.columns if c not in [\"quality\", \"label\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorAssembler:\n",
    "* Es una clase de pyspark.ml.feature que combina múltiples columnas (de tipo numérico) en un único vector. Este vector es necesario para muchos algoritmos de Machine Learning en PySpark.\n",
    "\n",
    "inputCols=feature_columns:\n",
    "* Define las columnas de entrada que se combinarán en el vector.\n",
    "En este caso, feature_columns es la lista de nombres de columnas creada previamente, que contiene las características (excluyendo \"quality\" y \"label\").\n",
    "\n",
    "outputCol=\"features\":\n",
    "\n",
    "* Especifica el nombre de la nueva columna donde se almacenará el vector de características. Aquí, se llamará \"features\".\n",
    "\n",
    "assembler.transform(df):\n",
    "* Aplica la transformación al DataFrame df. Esto genera una nueva columna llamada \"features\" que contiene el vector combinado para cada fila.\n",
    "\n",
    "df = ...:\n",
    "* Reasigna el DataFrame df con la nueva columna \"features\" incluida.\n",
    "\n",
    "Así, el DataFrame df ahora tiene una nueva columna llamada \"features\", que contiene un vector denso con los valores de todas las columnas de características para cada fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensamblaje de las características en un solo vector\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código utiliza la clase StandardScaler de PySpark para escalar (normalizar) las características del DataFrame. La normalización es importante en muchos modelos de Machine Learning porque garantiza que las características estén en la misma escala.\n",
    "\n",
    "StandardScaler:\n",
    "* Es una clase de pyspark.ml.feature que escala las características numéricas para que tengan:\n",
    "Media = 0 (si se usa centering, no activado por defecto).\n",
    "Varianza = 1.\n",
    "* Este proceso es útil para algoritmos que son sensibles a la magnitud de las características (por ejemplo, regresión logística o SVM).\n",
    "\n",
    "inputCol=\"features\":\n",
    "* Especifica la columna de entrada que contiene los vectores de características. En este caso, es la columna \"features\", creada anteriormente con VectorAssembler.\n",
    "\n",
    "outputCol=\"scaledFeatures\":\n",
    "* Especifica el nombre de la nueva columna que contendrá las características escaladas. En este caso, se llamará \"scaledFeatures\".\n",
    "\n",
    "scaler.fit(df):\n",
    "* Ajusta el escalador (StandardScaler) a los datos del DataFrame df. Esto calcula:\n",
    "- La media (si se activa el centrado).\n",
    "- La desviación estándar de las columnas en el vector de características.\n",
    "* Devuelve un modelo ajustado (scaler_model) que puede usarse para transformar los datos.\n",
    "\n",
    "scaler_model.transform(df):\n",
    "\n",
    "* Aplica el modelo ajustado para escalar las características en la columna \"features\".\n",
    "* Genera una nueva columna, \"scaledFeatures\", que contiene las características escaladas.\n",
    "\n",
    "df = ...:\n",
    "\n",
    "* Reasigna el DataFrame df con la nueva columna \"scaledFeatures\" incluida.\n",
    "\n",
    "El resultado es que el DataFrame df ahora incluye una nueva columna \"scaledFeatures\", que contiene los vectores escalados.\n",
    "Cada característica en \"scaledFeatures\" está normalizada con una desviación estándar de 1.\n",
    "\n",
    "Este paso asegura que todas las características tengan la misma importancia relativa en el modelo, evitando que las características con valores grandes dominen el aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarización de las características\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df)\n",
    "df = scaler_model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.randomSplit([0.8, 0.2], seed=42):\n",
    "\n",
    "* randomSplit: Método de PySpark que divide un DataFrame en varios subconjuntos de manera aleatoria.\n",
    "\n",
    "* [0.8, 0.2]: Define las proporciones de la división. Aquí:\n",
    "\n",
    "- El 80% de los datos se asigna al conjunto de entrenamiento (train_data).\n",
    "- El 20% de los datos se asigna al conjunto de prueba (test_data).\n",
    "seed=42: Especifica una semilla para el generador de números aleatorios. Esto garantiza que la división sea reproducible: cada vez que se ejecuta el código con la misma semilla, los subconjuntos serán iguales.\n",
    "\n",
    "train_data, test_data = ...:\n",
    "\n",
    "* Almacena los subconjuntos generados en las variables:\n",
    "- train_data: Contiene el 80% de los datos, utilizado para entrenar el modelo.\n",
    "- test_data: Contiene el 20% de los datos, utilizado para evaluar el modelo.\n",
    "\n",
    "La división en conjuntos de entrenamiento y prueba es crucial para evitar overfitting y evaluar el desempeño del modelo de manera objetiva. Esto permite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression:\n",
    "* Es una clase de pyspark.ml.classification que implementa el modelo de regresión logística.\n",
    "* La regresión logística es un modelo de clasificación lineal que predice la probabilidad de pertenecer a una clase \n",
    "\n",
    "featuresCol=\"scaledFeatures\":\n",
    "\n",
    "* Especifica que la columna \"scaledFeatures\" contiene los vectores de características que se utilizarán como entrada para el modelo.\n",
    "* Esta columna fue creada previamente al escalar las características originales.\n",
    "\n",
    "labelCol=\"label\":\n",
    "\n",
    "* Especifica que la columna \"label\" contiene las etiquetas de la variable objetivo.\n",
    "* Estas etiquetas son 0 o 1, creadas anteriormente a partir de la columna \"quality\".\n",
    "\n",
    "lr.fit(train_data):\n",
    "\n",
    "* Ajusta el modelo de regresión logística (lr) a los datos de entrenamiento (train_data).\n",
    "* Esto implica que el modelo aprende los pesos (coeficientes) para cada característica en \"scaledFeatures\" que mejor predicen la variable objetivo \"label\".\n",
    "\n",
    "lr_model:\n",
    "\n",
    "* Contiene el modelo entrenado. Este objeto almacena:\n",
    "Los coeficientes y el intercepto del modelo.\n",
    "Información sobre el rendimiento del ajuste en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo de regresión logística\n",
    "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr_model.transform(test_data):\n",
    "\n",
    "* Aplica el modelo entrenado (lr_model) al conjunto de datos de prueba (test_data).\n",
    "* Este proceso genera un nuevo DataFrame que incluye:\n",
    "- Las columnas originales del conjunto de prueba.\n",
    "- Nuevas columnas con los resultados de las predicciones.\n",
    "\n",
    "predictions:\n",
    "\n",
    "* Es el DataFrame resultante que contiene las predicciones realizadas por el modelo.\n",
    "\n",
    "predictions: Es un DataFrame que contiene tanto los datos originales como las predicciones y probabilidades generadas por el modelo para cada instancia en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. Evaluación del Modelo\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Este fragmento de código utiliza el evaluador BinaryClassificationEvaluator de PySpark para calcular el área bajo la curva ROC (AUC-ROC) de las predicciones realizadas por el modelo. La AUC-ROC mide la capacidad del modelo para distinguir entre las clases en un problema de clasificación binaria. \n",
    "\n",
    "BinaryClassificationEvaluator:\n",
    "\n",
    "* Es una clase de PySpark que evalúa el rendimiento de un modelo de clasificación binaria.\n",
    "* Calcula métricas basadas en las predicciones generadas.\n",
    "\n",
    "labelCol=\"label\":\n",
    "\n",
    "* Especifica que la columna \"label\" contiene las etiquetas verdaderas (0 o 1) de las clases.\n",
    "\n",
    "rawPredictionCol=\"rawPrediction\":\n",
    "\n",
    "* Especifica que la columna \"rawPrediction\" contiene los valores generados por el modelo antes de aplicar la función sigmoide o softmax.\n",
    "* Esta columna se utiliza para calcular métricas como el área bajo la curva ROC.\n",
    "\n",
    "metricName=\"areaUnderROC\":\n",
    "\n",
    "* Indica que el evaluador debe calcular la métrica de área bajo la curva ROC (AUC-ROC).\n",
    "\n",
    "evaluator.evaluate(predictions):\n",
    "\n",
    "* Calcula el valor de la métrica especificada (areaUnderROC) usando el DataFrame predictions, que contiene tanto las etiquetas verdaderas como las predicciones del modelo.\n",
    "\n",
    "roc_auc:\n",
    "\n",
    "* Contiene el valor calculado de AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Área bajo la curva ROC: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Evaluador para medir el área bajo la curva ROC\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"Área bajo la curva ROC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código utiliza el evaluador MulticlassClassificationEvaluator para calcular la exactitud (accuracy) del modelo en sus predicciones. La exactitud mide la proporción de predicciones correctas frente al total de predicciones realizadas. \n",
    "\n",
    "MulticlassClassificationEvaluator:\n",
    "\n",
    "* Es una clase de PySpark que evalúa el rendimiento de modelos de clasificación, ya sea binaria o multiclase.\n",
    "* Aunque en este caso el modelo es binario (0 o 1), este evaluador funciona igual de bien para clasificación binaria.\n",
    "\n",
    "labelCol=\"label\":\n",
    "\n",
    "* Especifica que la columna \"label\" contiene las etiquetas reales de las clases (0 o 1).\n",
    "\n",
    "predictionCol=\"prediction\":\n",
    "\n",
    "* Indica que la columna \"prediction\" contiene las predicciones del modelo para cada fila (0 o 1 en este caso).\n",
    "\n",
    "metricName=\"accuracy\": \n",
    "\n",
    "* Especifica que se debe calcular la métrica de exactitud: Predicciones correctas / total de predicciones \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Calcular la precisión y mostrar el informe de clasificación\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "print(f\"Exactitud del modelo: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|    1|       1.0|[0.01812103566953...|\n",
      "|    1|       1.0|[0.25500976345328...|\n",
      "|    0|       0.0|[0.79508949677236...|\n",
      "|    0|       0.0|[0.76399102429231...|\n",
      "|    1|       1.0|[0.04814519001736...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar una muestra de las predicciones\n",
    "predictions.select(\"label\", \"prediction\", \"probability\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProcesBigata",
   "language": "python",
   "name": "procesbigata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
